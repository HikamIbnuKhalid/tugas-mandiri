{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93403172",
   "metadata": {},
   "source": [
    "\n",
    "# Tugas Individu: Eksplorasi Autoencoder\n",
    "\n",
    "Notebook ini berisi eksperimen Autoencoder (Conv-AE, VAE, Beta-VAE) pada Fashion-MNIST.\n",
    "\n",
    "Tujuan: memahami dampak arsitektur encoder-decoder, dimensi laten, dan regularisasi pada kualitas rekonstruksi dan struktur latent.\n",
    "\n",
    "Jalankan sel berurutan. Notebook menggunakan PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup imports and config\n",
    "import os, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Output directory\n",
    "out_dir = Path('outputs_autoencoder')\n",
    "out_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset: Fashion-MNIST\n",
    "batch_size = 128\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Visual check\n",
    "imgs, labels = next(iter(train_loader))\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.axis('off')\n",
    "plt.imshow(np.transpose(make_grid(imgs[:16], nrow=4).cpu(), (1,2,0)).squeeze(), cmap='gray')\n",
    "plt.title('Sample Fashion-MNIST')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9bbffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model definitions: ConvAutoencoder, VAE (and reuse VAE for Beta-VAE by adjusting beta)\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1), # 28->14\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), # 14->7\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), # 7->4\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self._enc_out = 128 * 4 * 4\n",
    "        self.fc_enc = nn.Linear(self._enc_out, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, self._enc_out)\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.flatten(h)\n",
    "        z = self.fc_enc(h)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_dec(z)\n",
    "        h = h.view(-1, 128, 4, 4)\n",
    "        xrec = self.decoder_conv(h)\n",
    "        xrec = xrec[:, :, 2:30, 2:30]  # center crop to 28x28\n",
    "        return xrec\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=16):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self._enc_out = 128 * 4 * 4\n",
    "        self.fc_mu = nn.Linear(self._enc_out, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self._enc_out, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, self._enc_out)\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.flatten(h)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_dec(z)\n",
    "        h = h.view(-1, 128, 4, 4)\n",
    "        xrec = self.decoder_conv(h)\n",
    "        xrec = xrec[:, :, 2:30, 2:30]\n",
    "        return xrec\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9964883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss functions and training loops\n",
    "bce_loss = nn.BCELoss(reduction='sum')\n",
    "\n",
    "def loss_ae(recon_x, x):\n",
    "    return bce_loss(recon_x, x) / x.size(0)\n",
    "\n",
    "def loss_vae(recon_x, x, mu, logvar, beta=1.0):\n",
    "    BCE = bce_loss(recon_x, x)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (BCE + beta*KLD) / x.size(0), BCE / x.size(0), KLD / x.size(0)\n",
    "\n",
    "def train_ae(model, loader, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        xrec = model(x)\n",
    "        loss = loss_ae(xrec, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item()\n",
    "    avg = running / len(loader)\n",
    "    print(f'Epoch {epoch} AE train loss: {avg:.4f}')\n",
    "    return avg\n",
    "\n",
    "def eval_ae(model, loader, device):\n",
    "    model.eval()\n",
    "    running=0.0\n",
    "    with torch.no_grad():\n",
    "        for x,_ in loader:\n",
    "            x = x.to(device)\n",
    "            xrec = model(x)\n",
    "            running += loss_ae(xrec, x).item()\n",
    "    return running / len(loader)\n",
    "\n",
    "def train_vae(model, loader, optimizer, epoch, device, beta=1.0):\n",
    "    model.train()\n",
    "    running=0.0; bce=0.0; kld=0.0\n",
    "    for x,_ in loader:\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        xrec, mu, logvar = model(x)\n",
    "        loss, b, k = loss_vae(xrec, x, mu, logvar, beta)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item(); bce += b.item(); kld += k.item()\n",
    "    n = len(loader)\n",
    "    print(f'Epoch {epoch} VAE train loss: {running/n:.4f} BCE:{bce/n:.4f} KLD:{kld/n:.4f}')\n",
    "    return running/n, bce/n, kld/n\n",
    "\n",
    "def eval_vae(model, loader, device, beta=1.0):\n",
    "    model.eval()\n",
    "    running=0.0; bce=0.0; kld=0.0\n",
    "    with torch.no_grad():\n",
    "        for x,_ in loader:\n",
    "            x = x.to(device)\n",
    "            xrec, mu, logvar = model(x)\n",
    "            loss, b, k = loss_vae(xrec, x, mu, logvar, beta)\n",
    "            running += loss.item(); bce += b.item(); kld += k.item()\n",
    "    n = len(loader)\n",
    "    return running/n, bce/n, kld/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc73369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick experiment: ConvAE baseline (small epochs to keep runtime moderate)\n",
    "latent_dim = 32\n",
    "ae = ConvAutoencoder(latent_dim=latent_dim).to(device)\n",
    "opt = optim.Adam(ae.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 6\n",
    "train_losses = []; val_losses = []\n",
    "for e in range(1, n_epochs+1):\n",
    "    tr = train_ae(ae, train_loader, opt, e, device)\n",
    "    val = eval_ae(ae, test_loader, device)\n",
    "    train_losses.append(tr); val_losses.append(val)\n",
    "\n",
    "# Save model and plot losses\n",
    "torch.save(ae.state_dict(), out_dir / f'conv_ae_latent{latent_dim}.pt')\n",
    "\n",
    "plt.figure(); plt.plot(train_losses, label='train'); plt.plot(val_losses, label='val'); plt.legend(); plt.title('ConvAE Loss'); plt.show()\n",
    "\n",
    "# Show reconstructions\n",
    "def show_recon(model, loader, device, n=8, is_vae=False):\n",
    "    model.eval()\n",
    "    x,_ = next(iter(loader))\n",
    "    x = x[:n].to(device)\n",
    "    with torch.no_grad():\n",
    "        if is_vae:\n",
    "            xrec,_,_ = model(x)\n",
    "        else:\n",
    "            xrec = model(x)\n",
    "    grid = make_grid(torch.cat([x.cpu(), xrec.cpu()]), nrow=n)\n",
    "    plt.figure(figsize=(12,3)); plt.axis('off'); plt.imshow(np.transpose(grid,(1,2,0)).squeeze(), cmap='gray'); plt.show()\n",
    "\n",
    "show_recon(ae, test_loader, device, n=8, is_vae=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82803500",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VAE experiment\n",
    "latent_dim = 16\n",
    "vae = VAE(latent_dim=latent_dim).to(device)\n",
    "optv = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "n_epochs = 8\n",
    "for e in range(1, n_epochs+1):\n",
    "    train_vae(vae, train_loader, optv, e, device, beta=1.0)\n",
    "\n",
    "torch.save(vae.state_dict(), out_dir / f'vae_latent{latent_dim}.pt')\n",
    "# show recon\n",
    "def show_recon_vae(model, loader, device, n=8):\n",
    "    model.eval()\n",
    "    x,_ = next(iter(loader))\n",
    "    x = x[:n].to(device)\n",
    "    with torch.no_grad():\n",
    "        xrec, mu, logvar = model(x)\n",
    "    grid = make_grid(torch.cat([x.cpu(), xrec.cpu()]), nrow=n)\n",
    "    plt.figure(figsize=(12,3)); plt.axis('off'); plt.imshow(np.transpose(grid,(1,2,0)).squeeze(), cmap='gray'); plt.show()\n",
    "\n",
    "show_recon_vae(vae, test_loader, device, n=8)\n",
    "\n",
    "# Beta-VAE (beta=4)\n",
    "beta = 4.0\n",
    "bvae = VAE(latent_dim=latent_dim).to(device)\n",
    "optb = optim.Adam(bvae.parameters(), lr=1e-3)\n",
    "n_epochs = 8\n",
    "for e in range(1, n_epochs+1):\n",
    "    train_vae(bvae, train_loader, optb, e, device, beta=beta)\n",
    "\n",
    "torch.save(bvae.state_dict(), out_dir / f'beta_vae_latent{latent_dim}_beta{int(beta)}.pt')\n",
    "show_recon_vae(bvae, test_loader, device, n=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Latent interpolation utilities (works for AE and VAE)\n",
    "def interpolate_and_show(model, loader, device, steps=10, is_vae=False):\n",
    "    x,_ = next(iter(loader))\n",
    "    a = x[0:1].to(device); b = x[1:2].to(device)\n",
    "    if is_vae:\n",
    "        _, mu_a, _ = model(a); _, mu_b, _ = model(b)\n",
    "        za, zb = mu_a, mu_b\n",
    "    else:\n",
    "        za = model.encode(a); zb = model.encode(b)\n",
    "    imgs = []\n",
    "    for alpha in np.linspace(0,1,steps):\n",
    "        z = (1-alpha)*za + alpha*zb\n",
    "        xr = model.decode(z)\n",
    "        imgs.append(xr.cpu())\n",
    "    grid = make_grid(torch.cat(imgs, dim=0), nrow=steps)\n",
    "    plt.figure(figsize=(12,2)); plt.axis('off'); plt.imshow(np.transpose(grid,(1,2,0)).squeeze(), cmap='gray'); plt.title('Latent interpolation'); plt.show()\n",
    "\n",
    "# interpolation examples\n",
    "interpolate_and_show(ae, test_loader, device, steps=12, is_vae=False)\n",
    "interpolate_and_show(vae, test_loader, device, steps=12, is_vae=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfa2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick latent dimension sweep for ConvAE (small training to compare trend)\n",
    "latent_list = [8,16,32]\n",
    "results = {}\n",
    "for ld in latent_list:\n",
    "    m = ConvAutoencoder(latent_dim=ld).to(device)\n",
    "    optm = optim.Adam(m.parameters(), lr=1e-3)\n",
    "    for e in range(1,4):\n",
    "        train_ae(m, train_loader, optm, e, device)\n",
    "    # compute BCE per-batch average on test set\n",
    "    m.eval()\n",
    "    total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x,_ in test_loader:\n",
    "            x = x.to(device)\n",
    "            xrec = m(x)\n",
    "            total += bce_loss(xrec, x).item()\n",
    "    results[ld] = total / len(test_loader)\n",
    "print('Sweep results (lower better):', results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save a sample reconstruction image for the report\n",
    "x_sample, _ = next(iter(test_loader))\n",
    "x_sample = x_sample[:16].to(device)\n",
    "with torch.no_grad():\n",
    "    xrec = ae(x_sample)\n",
    "grid = make_grid(torch.cat([x_sample.cpu(), xrec.cpu()]), nrow=8)\n",
    "save_image(grid, out_dir / 'ae_reconstruction_grid.png')\n",
    "print('Saved reconstruction grid to', out_dir / 'ae_reconstruction_grid.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc3229",
   "metadata": {},
   "source": [
    "\n",
    "## Analisis & Refleksi\n",
    "\n",
    "- **Perbandingan**: ConvAE biasanya menghasilkan rekonstruksi paling tajam; VAE cenderung lebih blurrier namun memberikan latent space yang lebih teratur; Beta-VAE meningkatkan regularisasi tetapi mengorbankan detail.\n",
    "\n",
    "- **Kesulitan**: Menyeimbangkan loss (BCE vs KLD) untuk VAE; menyesuaikan output size di decoder (cropping pada convtransposes).\n",
    "\n",
    "- **Bantuan AI**: Jika Anda menggunakan bantuan AI untuk debugging atau saran arsitektur, cantumkan di bagian refleksi laporan.\n",
    "\n",
    "- **Saran pengembangan**: coba dataset lain (CelebA), augmentasi, dan eksperimen learning-rate schedule atau warmup untuk KLD.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
